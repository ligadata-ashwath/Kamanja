# zookeeper parameters
zookeeper.connect=127.0.0.1:2181
#zookeeper.session.timeout.ms=400
#zookeeper.sync.time.ms=200
 
# kafka parameters
kafka.group.id= hdfssink
kafka.topic=testavro

# These parameters will be passed to kafka consumer config.  
# offset storage can be kafka or zookeeper. default is zookeeper
#kafka.offsets.storage=kafka
# behavior if consumer offsets for above group.id are present. 
# can be smallest meaning read from beginning or largest meaning read only new messages
#kafka.auto.offset.reset=smallest

# number of parallel kafka consumers to run
consumer.threads=4

# implementation class to process messages
adapter.message.processor=com.ligadata.adapters.hdfs.BufferedPartitionedAvroSink

# uri to create files under
hdfs.uri=file:/tmp/data/instrumentationlog

# HDFS security settings for Kerberos login
#hdfs.keytabfile.key=
#hdfs.username.key=

# prefix to name all files created under above uri
file.prefix=AppLog

# can be deflate, snappy, bzip2, xz
# if not given, no compression is used
file.compression=bzip2

# SimpleDateFormat format string used to parse date in input message
input.date.format=yyyy-MM-dd

# partition messages using these comma separated ordered list of attributes
# format: attributename1,attributename2,..
# attribute names should match schema definition.
# optional SimpleDateFormat format string can be used after ":" for date attributes 
file.partition.strategy=timestamp:yyyy,timestamp:MM,timestamp:dd

# Avro schema file location
schema.file=src/main/resources/InstrumentationLog.avsc

# jdbc connection
jdbc.driver=com.microsoft.sqlserver.jdbc.SQLServerDriver
jdbc.url=jdbc:sqlserver://ec2-54-160-245-36.compute-1.amazonaws.com:1433;databaseName=admin;
jdbc.user=admin
jdbc.password=pass123
jdbc.insert.statement=
jdbc.update.statement=

# parameters to control message batching
# messages will be written every "count" messages or every "interval" seconds
sync.messages.count=10
sync.interval.seconds=120
