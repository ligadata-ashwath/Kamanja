# zookeeper parameters
zookeeper.connect=127.0.0.1:2181
#zookeeper.session.timeout.ms=400
#zookeeper.sync.time.ms=200
 
# kafka parameters
kafka.group.id= hdfssink
kafka.topic=cops_output

# These parameters will be passed to kafka consumer config.  
# offset storage can be kafka or zookeeper. default is zookeeper
#kafka.offsets.storage=kafka
# behavior if consumer offsets for above group.id are present. 
# can be smallest meaning read from beginning or largest meaning read only new messages
#kafka.auto.offset.reset=smallest

# number of parallel kafka consumers to run
consumer.threads=1

# implementation class to process messages
adapter.message.processor=com.ligadata.adapters.hdfs.BufferedPartitionedAvroSink

# uri to create files under
#hdfs.uri=file:/tmp/data/instrumentationlog
hdfs.uri=hdfs://localhost:9000/bofA/outputAdapter/instrumentationlog
#hdfs.kerberos.keytabfile=
#hdfs.kerberos.principal=
#hdfs.resource.file=

# prefix to name all files created under above uri
file.prefix=AppLog

# can be deflate, snappy, bzip2, xz
# if not given, no compression is used
file.compression=bzip2

# append or new 
file.mode=append

# SimpleDateFormat format string used to parse date in input message
input.date.format=yyyy-MM-dd

# partition messages using the of attributes in the format string
# attribute names should match schema definition.
# optional SimpleDateFormat format string can be used after ":" for date attributes 
file.partition.strategy=year=${timestamp:yyyy}/month=${timestamp:MM}/day=${timestamp:dd}

# Avro schema file location
#schema.file=src/main/resources/InstrumentationLog.avsc
schema.file=/opt/Kamanja-1.1.8/services/KafkaOutputAdapter/InstrumentationLog.avsc

# parameters to control message batching
# messages will be written every "count" messages or every "interval" seconds
sync.messages.count=10
sync.interval.seconds=120
